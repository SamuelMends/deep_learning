{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #importando nossa biblioteca Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores = pd.read_csv('entradas_breast.csv') #declarando nossas variáveis de entrada\n",
    "classe = pd.read_csv('saidas_breast.csv') #declarando nossas variáveis de saida (expectativa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split #Criando os ambientes de treinamento e teste\n",
    "previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, classe, test_size=0.25) #Declarando as variáveis de treinamento e de teste, em seguida splitando elas através do train_test_slipt, usando test_size = 25, indica que estamos utilizando apenas 25% de todos os registros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samue\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import keras #importando o Keras\n",
    "from keras.models import Sequential #Sequential é a modelo q vamos usar, possui esse nome pois é aformado pela sequência (entrada, primeira camada oculta, segunda camada oculta e saida)\n",
    "from keras.layers import Dense #Dense é o modelo de rede neural densa ou fully connected\n",
    "classificador = Sequential() #nossa rede neural se chama Sequential \n",
    "classificador.add(Dense(units=16, activation='relu', kernel_initializer='random_uniform', input_dim=30)) #Criando nossa primeira camada oculta. Units = Qtde de neuronios de entrada formula (qtde de entradas = 30 + qtde de saidas = 1) e divide por 2. O primeiro activation utilizamos o relu. Initializer é método pelo qual ele vai selecionar as entradas por isso utlizamos random. Input_dim é a qtde de elementos na camada de entrada do programa.\n",
    "classificador.add(Dense(units=16, activation='relu', kernel_initializer='random_uniform')) \n",
    "classificador.add(Dense(units=1, activation='sigmoid')) #Criando nossa camada de saida, units é a qtde de neurônios de saida e actvation nossa função de ativação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#otimizador = keras.optimizers.Adam(learning_rate = 0.001, weight_decay = 0.0001, clipvalue = 0.5)\n",
    "#classificador.compile(optimizer = otimizador, loss = 'binary_crossentropy', metrics= ['binary_accuracy'])\n",
    "classificador.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics= ['binary_accuracy']) #utilizando o otimizador ADAM para fazer o ajuste dos pesos (realiza a otimização da descida do gradiente stocastico). Loss binary crossentropy utilizamos essa função quando trabalhamos apenas com duas classes. Metrics= binary_accuracy para testar a acuracidade da nossa saida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 942us/step - binary_accuracy: 0.5891 - loss: 0.9701 \n",
      "Epoch 2/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - binary_accuracy: 0.6538 - loss: 0.6479\n",
      "Epoch 3/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807us/step - binary_accuracy: 0.6384 - loss: 0.5826\n",
      "Epoch 4/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807us/step - binary_accuracy: 0.7482 - loss: 0.4871\n",
      "Epoch 5/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 803us/step - binary_accuracy: 0.7500 - loss: 0.4607\n",
      "Epoch 6/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 801us/step - binary_accuracy: 0.8300 - loss: 0.4141\n",
      "Epoch 7/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 857us/step - binary_accuracy: 0.8352 - loss: 0.3933\n",
      "Epoch 8/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 844us/step - binary_accuracy: 0.8605 - loss: 0.3537\n",
      "Epoch 9/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - binary_accuracy: 0.8609 - loss: 0.3316\n",
      "Epoch 10/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 804us/step - binary_accuracy: 0.8936 - loss: 0.2952\n",
      "Epoch 11/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 853us/step - binary_accuracy: 0.8887 - loss: 0.2906\n",
      "Epoch 12/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 813us/step - binary_accuracy: 0.9029 - loss: 0.2825\n",
      "Epoch 13/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 795us/step - binary_accuracy: 0.8771 - loss: 0.2836\n",
      "Epoch 14/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 778us/step - binary_accuracy: 0.8790 - loss: 0.2908\n",
      "Epoch 15/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 834us/step - binary_accuracy: 0.8945 - loss: 0.2637\n",
      "Epoch 16/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 815us/step - binary_accuracy: 0.9095 - loss: 0.2646\n",
      "Epoch 17/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 912us/step - binary_accuracy: 0.9278 - loss: 0.2008\n",
      "Epoch 18/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 833us/step - binary_accuracy: 0.9403 - loss: 0.1863\n",
      "Epoch 19/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 826us/step - binary_accuracy: 0.8931 - loss: 0.2387\n",
      "Epoch 20/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 881us/step - binary_accuracy: 0.9340 - loss: 0.1955\n",
      "Epoch 21/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1000us/step - binary_accuracy: 0.8903 - loss: 0.2586\n",
      "Epoch 22/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8952 - loss: 0.2246\n",
      "Epoch 23/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 956us/step - binary_accuracy: 0.8778 - loss: 0.2601\n",
      "Epoch 24/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.8895 - loss: 0.2580 \n",
      "Epoch 25/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 856us/step - binary_accuracy: 0.9379 - loss: 0.1804\n",
      "Epoch 26/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step - binary_accuracy: 0.9304 - loss: 0.2036\n",
      "Epoch 27/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 841us/step - binary_accuracy: 0.9342 - loss: 0.1934\n",
      "Epoch 28/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9463 - loss: 0.1653\n",
      "Epoch 29/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - binary_accuracy: 0.9326 - loss: 0.2013\n",
      "Epoch 30/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9120 - loss: 0.1858 \n",
      "Epoch 31/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9241 - loss: 0.1790\n",
      "Epoch 32/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 932us/step - binary_accuracy: 0.9091 - loss: 0.2408\n",
      "Epoch 33/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9339 - loss: 0.1975 \n",
      "Epoch 34/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9269 - loss: 0.1999 \n",
      "Epoch 35/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 990us/step - binary_accuracy: 0.9030 - loss: 0.2453\n",
      "Epoch 36/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9254 - loss: 0.1999 \n",
      "Epoch 37/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9530 - loss: 0.1432 \n",
      "Epoch 38/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 959us/step - binary_accuracy: 0.9344 - loss: 0.1710\n",
      "Epoch 39/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9348 - loss: 0.1671 \n",
      "Epoch 40/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 970us/step - binary_accuracy: 0.9263 - loss: 0.1821\n",
      "Epoch 41/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 910us/step - binary_accuracy: 0.9260 - loss: 0.1943\n",
      "Epoch 42/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 977us/step - binary_accuracy: 0.9122 - loss: 0.2266\n",
      "Epoch 43/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9434 - loss: 0.1556 \n",
      "Epoch 44/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step - binary_accuracy: 0.9307 - loss: 0.2140\n",
      "Epoch 45/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 938us/step - binary_accuracy: 0.9406 - loss: 0.1808\n",
      "Epoch 46/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9229 - loss: 0.2037 \n",
      "Epoch 47/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9441 - loss: 0.1618 \n",
      "Epoch 48/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9239 - loss: 0.1638\n",
      "Epoch 49/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 941us/step - binary_accuracy: 0.9335 - loss: 0.1578\n",
      "Epoch 50/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 960us/step - binary_accuracy: 0.8946 - loss: 0.2293\n",
      "Epoch 51/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9677 - loss: 0.1329 \n",
      "Epoch 52/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 943us/step - binary_accuracy: 0.9413 - loss: 0.1690\n",
      "Epoch 53/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9079 - loss: 0.1985 \n",
      "Epoch 54/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9266 - loss: 0.1695 \n",
      "Epoch 55/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9347 - loss: 0.1564 \n",
      "Epoch 56/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937us/step - binary_accuracy: 0.9249 - loss: 0.1514\n",
      "Epoch 57/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9367 - loss: 0.1553 \n",
      "Epoch 58/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 962us/step - binary_accuracy: 0.9207 - loss: 0.1821\n",
      "Epoch 59/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 942us/step - binary_accuracy: 0.9458 - loss: 0.1299\n",
      "Epoch 60/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9166 - loss: 0.1984 \n",
      "Epoch 61/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9361 - loss: 0.1683 \n",
      "Epoch 62/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9535 - loss: 0.1458 \n",
      "Epoch 63/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 992us/step - binary_accuracy: 0.9424 - loss: 0.1474\n",
      "Epoch 64/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9302 - loss: 0.1665\n",
      "Epoch 65/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 838us/step - binary_accuracy: 0.9230 - loss: 0.1753\n",
      "Epoch 66/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 905us/step - binary_accuracy: 0.9400 - loss: 0.1311\n",
      "Epoch 67/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9424 - loss: 0.1489 \n",
      "Epoch 68/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 986us/step - binary_accuracy: 0.9310 - loss: 0.1491\n",
      "Epoch 69/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 910us/step - binary_accuracy: 0.9529 - loss: 0.1321\n",
      "Epoch 70/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9729 - loss: 0.0785 \n",
      "Epoch 71/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 975us/step - binary_accuracy: 0.9214 - loss: 0.1831\n",
      "Epoch 72/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 981us/step - binary_accuracy: 0.9374 - loss: 0.1634\n",
      "Epoch 73/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9389 - loss: 0.1483 \n",
      "Epoch 74/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 953us/step - binary_accuracy: 0.9599 - loss: 0.1285\n",
      "Epoch 75/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9502 - loss: 0.1139 \n",
      "Epoch 76/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 974us/step - binary_accuracy: 0.9564 - loss: 0.1235\n",
      "Epoch 77/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9372 - loss: 0.1827 \n",
      "Epoch 78/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 999us/step - binary_accuracy: 0.9593 - loss: 0.1184\n",
      "Epoch 79/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9673 - loss: 0.1138 \n",
      "Epoch 80/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9234 - loss: 0.1523 \n",
      "Epoch 81/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9426 - loss: 0.1598\n",
      "Epoch 82/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9486 - loss: 0.1435 \n",
      "Epoch 83/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9520 - loss: 0.1321 \n",
      "Epoch 84/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 983us/step - binary_accuracy: 0.9400 - loss: 0.1296\n",
      "Epoch 85/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9333 - loss: 0.1385 \n",
      "Epoch 86/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 935us/step - binary_accuracy: 0.9528 - loss: 0.1169\n",
      "Epoch 87/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9310 - loss: 0.1588 \n",
      "Epoch 88/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 962us/step - binary_accuracy: 0.9417 - loss: 0.1205\n",
      "Epoch 89/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 989us/step - binary_accuracy: 0.9632 - loss: 0.1137\n",
      "Epoch 90/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9321 - loss: 0.1424 \n",
      "Epoch 91/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9518 - loss: 0.0977 \n",
      "Epoch 92/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9512 - loss: 0.1192\n",
      "Epoch 93/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9573 - loss: 0.1120 \n",
      "Epoch 94/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9531 - loss: 0.0994 \n",
      "Epoch 95/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9351 - loss: 0.1455 \n",
      "Epoch 96/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 905us/step - binary_accuracy: 0.9665 - loss: 0.0860\n",
      "Epoch 97/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9350 - loss: 0.1386 \n",
      "Epoch 98/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9336 - loss: 0.1272\n",
      "Epoch 99/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 919us/step - binary_accuracy: 0.9622 - loss: 0.0917\n",
      "Epoch 100/100\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - binary_accuracy: 0.9520 - loss: 0.0959 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x20f73b70ce0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classificador.fit(previsores_treinamento, classe_treinamento, batch_size = 10, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 3.51863474e-01,  4.96380515e-02,  2.64677741e-02,\n",
      "        -3.27161178e-02,  2.20746230e-02,  6.01817891e-02,\n",
      "         7.52324052e-03, -9.75807011e-02,  9.88974236e-03,\n",
      "        -4.06668261e-02, -1.41672953e-03,  3.56142856e-02,\n",
      "         2.22021073e-01,  4.85404618e-02, -2.74881367e-02,\n",
      "         8.52232203e-02],\n",
      "       [ 9.36570764e-02,  9.09367278e-02,  1.83866341e-02,\n",
      "         4.27234173e-03,  3.48690599e-02,  1.51006579e-01,\n",
      "        -3.12056959e-01, -1.88989639e-01, -2.46155988e-02,\n",
      "        -4.73936535e-02, -1.05966479e-01,  7.42572322e-02,\n",
      "         6.72840774e-02,  2.15989258e-02, -2.64355130e-02,\n",
      "        -5.85397705e-02],\n",
      "       [ 3.34338695e-01,  2.21831407e-02, -4.06046622e-02,\n",
      "        -9.30988789e-03, -2.99697593e-02,  2.57740200e-01,\n",
      "        -1.56119853e-01, -6.14703596e-02, -1.18120750e-02,\n",
      "        -3.89895104e-02, -2.24867046e-01,  5.56120323e-03,\n",
      "         2.13692397e-01, -9.01069678e-03,  2.19501164e-02,\n",
      "         2.57186502e-01],\n",
      "       [ 1.13426819e-01,  5.15684485e-02, -2.45573930e-02,\n",
      "        -5.37595898e-03, -6.01960681e-02, -1.08370837e-02,\n",
      "         5.50152101e-02,  9.78768244e-03,  1.37151685e-02,\n",
      "        -2.24448573e-02, -8.43270682e-03, -1.35471264e-03,\n",
      "        -1.85053684e-02, -5.44445403e-02, -2.99577042e-02,\n",
      "         3.41065712e-02],\n",
      "       [-1.34585381e-01,  8.33047405e-02, -2.96510067e-02,\n",
      "         2.92263515e-02,  1.53003410e-02,  5.87648386e-03,\n",
      "         2.90751439e-02, -1.40214385e-02, -1.44129861e-02,\n",
      "         1.76381916e-02,  7.62374103e-02, -6.88913986e-02,\n",
      "         2.44533911e-01,  2.94207130e-02, -2.24243402e-02,\n",
      "        -9.68120173e-02],\n",
      "       [-2.87234355e-02,  3.80124360e-01,  6.73892582e-03,\n",
      "         2.92458273e-02, -4.84700762e-02, -1.22568287e-01,\n",
      "         2.57305074e-02,  7.86490217e-02, -3.44403386e-02,\n",
      "         2.80534625e-02,  1.56232193e-01, -8.42796117e-02,\n",
      "         2.00744495e-01,  4.58312295e-02, -1.60423368e-02,\n",
      "        -3.23170930e-01],\n",
      "       [-1.56930424e-02,  1.56595588e-01,  3.09888646e-03,\n",
      "        -1.69240311e-03,  5.84019274e-02, -1.13180146e-01,\n",
      "        -9.42370575e-03, -2.31930744e-02,  3.00389640e-02,\n",
      "        -2.61665846e-04,  1.38274774e-01, -4.01424505e-02,\n",
      "        -1.20234832e-01, -4.52791452e-02,  5.25534414e-02,\n",
      "         1.61915764e-01],\n",
      "       [ 1.08127937e-01, -2.90894997e-03,  1.63914133e-02,\n",
      "        -4.39038165e-02,  2.95070224e-02,  7.88495168e-02,\n",
      "         5.08125722e-02,  2.82390341e-02,  4.55911681e-02,\n",
      "        -4.17853445e-02, -6.45180643e-02, -1.11912712e-02,\n",
      "         2.22481057e-01, -9.02477838e-03, -2.69362293e-02,\n",
      "         9.34489667e-02],\n",
      "       [ 9.35604349e-02,  4.43461031e-01, -5.19801909e-03,\n",
      "         2.16196440e-02, -2.43633613e-03,  2.74012480e-02,\n",
      "        -2.07148671e-01,  5.75232273e-03, -5.72062507e-02,\n",
      "         1.33034527e-01,  3.12319789e-02, -4.48873788e-02,\n",
      "         5.75324381e-03, -3.58472466e-02,  4.82618250e-02,\n",
      "        -3.30274761e-01],\n",
      "       [ 4.46552962e-01, -4.88409288e-02,  3.44188977e-03,\n",
      "        -5.76070696e-03, -1.07890822e-01, -1.89259872e-01,\n",
      "         3.14678162e-01,  6.03958033e-02, -1.09545393e-02,\n",
      "         3.68050970e-02, -2.60839909e-01,  2.03807391e-02,\n",
      "        -3.05563003e-01, -1.54787982e-02,  6.74861446e-02,\n",
      "         1.01108424e-01],\n",
      "       [ 4.29072343e-02, -7.82647803e-02, -4.42534052e-02,\n",
      "         4.73455228e-02, -1.82783417e-02,  1.82155184e-02,\n",
      "         1.33396253e-01,  7.00721890e-02, -2.83496752e-02,\n",
      "        -4.95631294e-03,  6.30835630e-03, -5.66030592e-02,\n",
      "        -6.05678931e-02,  2.76464168e-02,  4.12998088e-02,\n",
      "        -1.64463192e-01],\n",
      "       [-2.15696674e-02, -7.61288393e-04, -1.06267966e-02,\n",
      "        -2.50629336e-03,  1.89914424e-02,  1.61171798e-02,\n",
      "         2.18888447e-02, -6.03737161e-02, -1.70184672e-02,\n",
      "        -3.17022437e-03, -3.06359958e-02, -2.44494919e-02,\n",
      "         5.88303022e-02, -6.46891887e-04, -5.98281296e-03,\n",
      "        -2.44473387e-02],\n",
      "       [ 5.78250410e-03, -1.93777680e-02, -2.41018347e-02,\n",
      "        -4.58523743e-02, -1.90592259e-02, -5.03173098e-03,\n",
      "        -8.49638060e-02, -6.84940070e-02, -1.03517994e-03,\n",
      "        -2.27298401e-02,  3.76835391e-02, -3.51027288e-02,\n",
      "        -7.95667991e-03, -4.43750643e-04, -2.42056008e-02,\n",
      "         3.23625207e-02],\n",
      "       [-1.69426978e-01, -3.48074622e-02,  6.23659901e-02,\n",
      "        -2.22938191e-02, -1.10026315e-01, -1.63455054e-01,\n",
      "         7.25886151e-02, -7.39556691e-03,  5.12351766e-02,\n",
      "         6.58420101e-02,  8.60992372e-02, -2.61120088e-02,\n",
      "        -1.14766546e-01, -1.83485784e-02,  4.05094065e-02,\n",
      "        -2.08527461e-01],\n",
      "       [ 1.71920687e-01,  1.17141291e-01, -1.45748183e-02,\n",
      "         4.26479317e-02,  3.92661430e-02,  1.43520519e-01,\n",
      "        -2.58544445e-01,  6.52646422e-02,  6.62965700e-02,\n",
      "        -3.44992988e-02, -1.23634212e-01,  4.85247187e-02,\n",
      "         1.17509298e-01, -1.92728844e-02,  1.80803929e-02,\n",
      "         6.76350072e-02],\n",
      "       [ 7.78753683e-02,  5.41152120e-01, -1.09313773e-02,\n",
      "        -5.41205332e-03,  2.00235769e-01,  1.75648451e-01,\n",
      "        -4.36420918e-01, -3.23708006e-03, -6.29357547e-02,\n",
      "        -3.14753167e-02, -9.74744931e-02,  8.45923275e-03,\n",
      "         3.88660401e-01,  3.42658497e-02, -4.46376242e-02,\n",
      "        -3.34925264e-01],\n",
      "       [-2.18421474e-01, -1.81966666e-02, -7.04852073e-03,\n",
      "        -2.08899863e-02, -3.87825556e-02, -2.27300804e-02,\n",
      "         2.51213508e-03,  3.33630204e-01, -1.73938777e-02,\n",
      "        -2.93025989e-02,  5.75220212e-02, -3.35522671e-03,\n",
      "        -5.39699912e-01,  3.39902528e-02, -4.40070443e-02,\n",
      "        -1.54882953e-01],\n",
      "       [ 2.75047719e-01,  1.05785169e-01, -5.46084754e-02,\n",
      "         1.36286132e-02, -7.11461082e-02,  1.48501292e-01,\n",
      "         2.82084465e-01,  2.29541987e-01, -2.94734482e-02,\n",
      "        -1.04601197e-02, -2.03477517e-01, -3.73825319e-02,\n",
      "         2.17869759e-01, -5.68977445e-02, -1.67199429e-02,\n",
      "         3.86056185e-01],\n",
      "       [ 1.92092910e-01, -3.83759439e-01,  2.94491313e-02,\n",
      "         4.97500785e-02, -1.75891910e-02, -4.08685714e-01,\n",
      "         5.82278430e-01,  3.88232797e-01, -6.02972880e-03,\n",
      "         4.30609025e-02, -1.08779281e-01,  8.72829743e-03,\n",
      "        -5.48965991e-01, -2.25832220e-02,  2.15014834e-02,\n",
      "         2.44491413e-01],\n",
      "       [-1.64671481e-01,  1.56843916e-01, -4.54117581e-02,\n",
      "        -1.74989328e-02, -2.63350271e-02, -2.11163878e-01,\n",
      "         2.52101123e-01,  2.21367195e-01,  3.16735432e-02,\n",
      "         3.44865732e-02,  3.84848386e-01, -2.00262405e-02,\n",
      "        -3.35398585e-01, -4.43867929e-02,  2.98487544e-02,\n",
      "        -5.96892275e-02],\n",
      "       [ 2.50475496e-01,  1.07117137e-02,  3.04871630e-02,\n",
      "        -3.71250622e-02,  3.45948152e-02,  3.85305174e-02,\n",
      "         3.31055447e-02, -2.37689316e-02,  3.38733755e-02,\n",
      "         4.91296919e-03, -6.64566085e-02,  2.86177192e-02,\n",
      "         1.68241277e-01,  3.93387228e-02, -5.19964732e-02,\n",
      "         1.56682134e-01],\n",
      "       [-1.19465828e-01, -1.48236603e-02, -2.36009862e-02,\n",
      "        -8.94138962e-03, -2.10447218e-02, -1.08622527e-02,\n",
      "        -3.28918159e-01, -1.30158454e-01, -1.95189065e-03,\n",
      "        -9.46522411e-03,  8.76181126e-02,  3.93947326e-02,\n",
      "        -2.67945658e-02,  4.21322323e-02,  3.22430441e-03,\n",
      "        -2.61917770e-01],\n",
      "       [ 2.39483178e-01, -7.35545065e-03, -3.46903093e-02,\n",
      "         1.21001378e-02, -4.03931439e-02,  1.79586336e-01,\n",
      "        -1.12407662e-01,  4.35263626e-02,  4.63800244e-02,\n",
      "         2.18726192e-02, -1.60002589e-01,  3.95495109e-02,\n",
      "         1.41165555e-01, -2.65067853e-02, -2.26901751e-02,\n",
      "         1.15610041e-01],\n",
      "       [-1.31689474e-01,  1.38863595e-02, -6.91697281e-03,\n",
      "        -4.26889770e-02, -3.93567979e-02, -3.39576416e-02,\n",
      "         4.66848724e-02,  7.48294592e-02, -4.49481010e-02,\n",
      "         5.79745928e-03,  7.88461417e-02, -2.29861233e-02,\n",
      "        -5.75341359e-02, -2.78893709e-02, -4.91834916e-02,\n",
      "        -1.30308330e-01],\n",
      "       [-3.85327637e-02, -2.51145780e-01, -3.58021306e-03,\n",
      "        -4.79727164e-02,  7.87406638e-02,  4.17267419e-02,\n",
      "        -2.35513285e-01,  7.17036426e-02, -3.41328122e-02,\n",
      "         1.37114786e-02,  1.26416370e-01, -3.47613804e-02,\n",
      "        -1.78230539e-01, -2.05987636e-02, -3.44684417e-03,\n",
      "         1.54710680e-01],\n",
      "       [ 9.53338202e-03,  1.39564976e-01,  1.27544664e-02,\n",
      "        -1.54156312e-02, -2.45194100e-02, -5.16538024e-02,\n",
      "        -1.82172447e-01, -3.93882990e-02,  1.33909937e-02,\n",
      "         3.09621729e-02,  7.96180144e-02,  3.23228836e-02,\n",
      "        -9.92576708e-04,  2.56107897e-02, -4.47890349e-02,\n",
      "        -1.78320352e-02],\n",
      "       [ 1.49517022e-02, -7.07320124e-02,  3.70340571e-02,\n",
      "        -1.29111037e-02, -7.71850068e-03,  4.88611870e-02,\n",
      "        -1.90345541e-01, -2.36012489e-02, -5.02854884e-02,\n",
      "        -5.35936374e-03,  1.33310318e-01,  3.07466928e-02,\n",
      "         1.31355122e-01, -1.85580216e-02,  2.83662938e-02,\n",
      "         1.21359773e-01],\n",
      "       [-2.37914085e-01,  3.18258762e-01, -5.60401604e-02,\n",
      "         4.68797125e-02, -3.61261405e-02, -1.00920364e-01,\n",
      "         2.71741650e-03,  1.54506847e-01, -3.36590633e-02,\n",
      "         1.87504534e-02,  2.87450776e-02,  4.93567158e-03,\n",
      "         7.09648523e-03, -4.13150527e-02, -3.71771008e-02,\n",
      "         3.10922340e-02],\n",
      "       [ 8.59788153e-03,  2.79892325e-01, -2.07228232e-02,\n",
      "         2.14735419e-03,  1.16482498e-02, -5.29228151e-02,\n",
      "        -1.66716650e-01, -6.14573285e-02, -1.37229068e-02,\n",
      "        -1.00014266e-02, -1.91370323e-02,  8.38801265e-02,\n",
      "        -4.78761420e-02, -3.04410956e-03,  3.31877284e-02,\n",
      "        -3.25377993e-02],\n",
      "       [-2.47645646e-01,  1.15488388e-01, -4.84108590e-02,\n",
      "        -4.17833552e-02, -4.12973315e-02, -2.17008442e-01,\n",
      "         2.11728603e-01,  3.05290610e-01, -4.95105349e-02,\n",
      "        -3.63257490e-02,  2.47377872e-01, -5.77277653e-02,\n",
      "        -2.08079264e-01,  3.35746221e-02, -4.59984504e-02,\n",
      "        -2.29903176e-01]], dtype=float32), array([ 0.6842779 ,  0.05786775, -0.00779152,  0.        ,  0.04647836,\n",
      "        0.46069285, -0.3427369 , -0.11115764,  0.03840898,  0.00644259,\n",
      "       -0.5588445 ,  0.05645712,  0.44067317, -0.00671176,  0.00077076,\n",
      "        0.54308575], dtype=float32)]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "pesos0 = classificador.layers[0].get_weights()\n",
    "print(pesos0)\n",
    "print(len(pesos0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000020F7F0E2AC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 55ms/stepWARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000020F7F0E2AC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n"
     ]
    }
   ],
   "source": [
    "previsoes = classificador.predict(previsores_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 880us/step - binary_accuracy: 0.9015 - loss: 0.2575\n"
     ]
    }
   ],
   "source": [
    "resultado = classificador.evaluate(previsores_teste, classe_teste)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
