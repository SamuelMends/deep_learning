{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #importando nossa biblioteca Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores = pd.read_csv('entradas_breast.csv') #declarando nossas variáveis de entrada\n",
    "classe = pd.read_csv('saidas_breast.csv') #declarando nossas variáveis de saida (expectativa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split #Criando os ambientes de treinamento e teste\n",
    "previsores_treinamento, previsores_teste, classe_treinamento, classe_teste = train_test_split(previsores, classe, test_size=0.25) #Declarando as variáveis de treinamento e de teste, em seguida splitando elas através do train_test_slipt, usando test_size = 25, indica que estamos utilizando apenas 25% de todos os registros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samue\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import keras #importando o Keras\n",
    "from keras.models import Sequential #Sequential é a modelo q vamos usar, possui esse nome pois é aformado pela sequência (entrada, primeira camada oculta, segunda camada oculta e saida)\n",
    "from keras.layers import Dense #Dense é o modelo de rede neural densa ou fully connected\n",
    "classificador = Sequential() #nossa rede neural se chama Sequential \n",
    "classificador.add(Dense(units=16, activation='relu', kernel_initializer='random_uniform', input_dim=30)) #Criando nossa primeira camada oculta. Units = Qtde de neuronios de entrada formula (qtde de entradas = 30 + qtde de saidas = 1) e divide por 2. O primeiro activation utilizamos o relu. Initializer é método pelo qual ele vai selecionar as entradas por isso utlizamos random. Input_dim é a qtde de elementos na camada de entrada do programa.\n",
    "classificador.add(Dense(units=16, activation='relu', kernel_initializer='random_uniform')) \n",
    "classificador.add(Dense(units=1, activation='sigmoid')) #Criando nossa camada de saida, units é a qtde de neurônios de saida e actvation nossa função de ativação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#otimizador = keras.optimizers.Adam(learning_rate = 0.001, weight_decay = 0.0001, clipvalue = 0.5)\n",
    "#classificador.compile(optimizer = otimizador, loss = 'binary_crossentropy', metrics= ['binary_accuracy'])\n",
    "classificador.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics= ['binary_accuracy']) #utilizando o otimizador ADAM para fazer o ajuste dos pesos (realiza a otimização da descida do gradiente stocastico). Loss binary crossentropy utilizamos essa função quando trabalhamos apenas com duas classes. Metrics= binary_accuracy para testar a acuracidade da nossa saida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - binary_accuracy: 0.6428 - loss: 0.7412\n",
      "Epoch 2/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.7076 - loss: 0.4933\n",
      "Epoch 3/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8041 - loss: 0.3888\n",
      "Epoch 4/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.7990 - loss: 0.4284\n",
      "Epoch 5/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8541 - loss: 0.3468\n",
      "Epoch 6/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8433 - loss: 0.3436\n",
      "Epoch 7/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8312 - loss: 0.3638\n",
      "Epoch 8/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8505 - loss: 0.3361\n",
      "Epoch 9/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9019 - loss: 0.2567\n",
      "Epoch 10/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8412 - loss: 0.3301\n",
      "Epoch 11/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9229 - loss: 0.2332\n",
      "Epoch 12/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.8916 - loss: 0.2638\n",
      "Epoch 13/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9184 - loss: 0.2355\n",
      "Epoch 14/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9127 - loss: 0.2010\n",
      "Epoch 15/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9308 - loss: 0.2176\n",
      "Epoch 16/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8938 - loss: 0.2487\n",
      "Epoch 17/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9058 - loss: 0.2287\n",
      "Epoch 18/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9179 - loss: 0.2052\n",
      "Epoch 19/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9183 - loss: 0.1862\n",
      "Epoch 20/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9345 - loss: 0.1941\n",
      "Epoch 21/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9354 - loss: 0.1791\n",
      "Epoch 22/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9161 - loss: 0.1823\n",
      "Epoch 23/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9119 - loss: 0.2312\n",
      "Epoch 24/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9256 - loss: 0.2007\n",
      "Epoch 25/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9195 - loss: 0.1929\n",
      "Epoch 26/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9207 - loss: 0.1869\n",
      "Epoch 27/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9143 - loss: 0.1721\n",
      "Epoch 28/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9059 - loss: 0.2121\n",
      "Epoch 29/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9365 - loss: 0.1930\n",
      "Epoch 30/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9427 - loss: 0.1668\n",
      "Epoch 31/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9420 - loss: 0.1633\n",
      "Epoch 32/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8878 - loss: 0.2390\n",
      "Epoch 33/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9326 - loss: 0.1682\n",
      "Epoch 34/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9171 - loss: 0.2215\n",
      "Epoch 35/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9318 - loss: 0.1860\n",
      "Epoch 36/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9055 - loss: 0.2278\n",
      "Epoch 37/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9288 - loss: 0.2070\n",
      "Epoch 38/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9252 - loss: 0.1621\n",
      "Epoch 39/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9360 - loss: 0.1570\n",
      "Epoch 40/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9397 - loss: 0.1527\n",
      "Epoch 41/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9531 - loss: 0.1338\n",
      "Epoch 42/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.8895 - loss: 0.2037\n",
      "Epoch 43/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9383 - loss: 0.1562\n",
      "Epoch 44/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9276 - loss: 0.1677\n",
      "Epoch 45/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9140 - loss: 0.2045\n",
      "Epoch 46/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9151 - loss: 0.2359\n",
      "Epoch 47/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9017 - loss: 0.2544\n",
      "Epoch 48/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9349 - loss: 0.1763\n",
      "Epoch 49/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9442 - loss: 0.1601\n",
      "Epoch 50/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9416 - loss: 0.1350\n",
      "Epoch 51/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9546 - loss: 0.1395\n",
      "Epoch 52/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9460 - loss: 0.1325\n",
      "Epoch 53/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9501 - loss: 0.1274\n",
      "Epoch 54/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9212 - loss: 0.1866\n",
      "Epoch 55/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9412 - loss: 0.1466\n",
      "Epoch 56/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9439 - loss: 0.1422\n",
      "Epoch 57/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9313 - loss: 0.1646\n",
      "Epoch 58/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9397 - loss: 0.1718\n",
      "Epoch 59/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8852 - loss: 0.2679\n",
      "Epoch 60/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9383 - loss: 0.1708\n",
      "Epoch 61/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9509 - loss: 0.1376\n",
      "Epoch 62/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9345 - loss: 0.1325\n",
      "Epoch 63/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9341 - loss: 0.1714\n",
      "Epoch 64/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9375 - loss: 0.1655\n",
      "Epoch 65/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9365 - loss: 0.1321\n",
      "Epoch 66/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9379 - loss: 0.1283\n",
      "Epoch 67/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9376 - loss: 0.1409\n",
      "Epoch 68/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9533 - loss: 0.1289\n",
      "Epoch 69/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9559 - loss: 0.1203\n",
      "Epoch 70/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9322 - loss: 0.1607\n",
      "Epoch 71/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9557 - loss: 0.1060\n",
      "Epoch 72/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8965 - loss: 0.2837\n",
      "Epoch 73/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9482 - loss: 0.1274\n",
      "Epoch 74/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9520 - loss: 0.1090\n",
      "Epoch 75/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9333 - loss: 0.1541\n",
      "Epoch 76/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9505 - loss: 0.1152\n",
      "Epoch 77/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9173 - loss: 0.1932\n",
      "Epoch 78/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9240 - loss: 0.1542\n",
      "Epoch 79/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9436 - loss: 0.1483\n",
      "Epoch 80/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9452 - loss: 0.1495\n",
      "Epoch 81/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9555 - loss: 0.1098\n",
      "Epoch 82/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9338 - loss: 0.1499\n",
      "Epoch 83/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9465 - loss: 0.1523\n",
      "Epoch 84/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9423 - loss: 0.1391\n",
      "Epoch 85/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9532 - loss: 0.1137\n",
      "Epoch 86/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9541 - loss: 0.1344\n",
      "Epoch 87/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - binary_accuracy: 0.9465 - loss: 0.1265\n",
      "Epoch 88/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9479 - loss: 0.1114\n",
      "Epoch 89/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9552 - loss: 0.1249\n",
      "Epoch 90/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9612 - loss: 0.0949\n",
      "Epoch 91/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9464 - loss: 0.1316\n",
      "Epoch 92/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8861 - loss: 0.2599\n",
      "Epoch 93/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9170 - loss: 0.1833\n",
      "Epoch 94/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9708 - loss: 0.0843\n",
      "Epoch 95/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9517 - loss: 0.1067\n",
      "Epoch 96/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9492 - loss: 0.1296\n",
      "Epoch 97/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9167 - loss: 0.1609\n",
      "Epoch 98/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9617 - loss: 0.1036\n",
      "Epoch 99/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9582 - loss: 0.0965\n",
      "Epoch 100/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9517 - loss: 0.1118\n",
      "Epoch 101/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9354 - loss: 0.1207\n",
      "Epoch 102/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9705 - loss: 0.1189\n",
      "Epoch 103/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9465 - loss: 0.1211\n",
      "Epoch 104/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9484 - loss: 0.1116\n",
      "Epoch 105/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9703 - loss: 0.0922\n",
      "Epoch 106/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9471 - loss: 0.1346\n",
      "Epoch 107/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9527 - loss: 0.1366\n",
      "Epoch 108/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9568 - loss: 0.1024\n",
      "Epoch 109/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9343 - loss: 0.1537\n",
      "Epoch 110/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9495 - loss: 0.1200\n",
      "Epoch 111/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9510 - loss: 0.1194\n",
      "Epoch 112/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9594 - loss: 0.0838\n",
      "Epoch 113/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9715 - loss: 0.0907\n",
      "Epoch 114/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9566 - loss: 0.0978\n",
      "Epoch 115/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9611 - loss: 0.0754\n",
      "Epoch 116/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9717 - loss: 0.0988\n",
      "Epoch 117/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9635 - loss: 0.1160\n",
      "Epoch 118/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9625 - loss: 0.1128\n",
      "Epoch 119/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9335 - loss: 0.1146\n",
      "Epoch 120/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9244 - loss: 0.1848\n",
      "Epoch 121/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9463 - loss: 0.1087\n",
      "Epoch 122/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9590 - loss: 0.1233\n",
      "Epoch 123/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9425 - loss: 0.1247\n",
      "Epoch 124/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9426 - loss: 0.1265\n",
      "Epoch 125/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9791 - loss: 0.0643\n",
      "Epoch 126/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9577 - loss: 0.0802\n",
      "Epoch 127/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9690 - loss: 0.0700\n",
      "Epoch 128/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9423 - loss: 0.1105\n",
      "Epoch 129/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9426 - loss: 0.1240\n",
      "Epoch 130/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9637 - loss: 0.0963\n",
      "Epoch 131/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9477 - loss: 0.0903 \n",
      "Epoch 132/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9437 - loss: 0.1217\n",
      "Epoch 133/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9707 - loss: 0.0738    \n",
      "Epoch 134/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9437 - loss: 0.1268\n",
      "Epoch 135/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9604 - loss: 0.0921\n",
      "Epoch 136/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9670 - loss: 0.0850\n",
      "Epoch 137/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9633 - loss: 0.1012\n",
      "Epoch 138/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9591 - loss: 0.0827\n",
      "Epoch 139/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9583 - loss: 0.0979\n",
      "Epoch 140/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9538 - loss: 0.1061\n",
      "Epoch 141/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9747 - loss: 0.0690\n",
      "Epoch 142/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9678 - loss: 0.0717\n",
      "Epoch 143/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9584 - loss: 0.0631\n",
      "Epoch 144/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9429 - loss: 0.1076\n",
      "Epoch 145/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.8745 - loss: 0.2761\n",
      "Epoch 146/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9413 - loss: 0.1353\n",
      "Epoch 147/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9486 - loss: 0.1290\n",
      "Epoch 148/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9421 - loss: 0.1500\n",
      "Epoch 149/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9462 - loss: 0.1440\n",
      "Epoch 150/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9707 - loss: 0.0764\n",
      "Epoch 151/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9666 - loss: 0.0697\n",
      "Epoch 152/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9754 - loss: 0.0836\n",
      "Epoch 153/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9536 - loss: 0.0895\n",
      "Epoch 154/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9565 - loss: 0.1052\n",
      "Epoch 155/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9570 - loss: 0.0928\n",
      "Epoch 156/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9420 - loss: 0.1428\n",
      "Epoch 157/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9414 - loss: 0.1309\n",
      "Epoch 158/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9583 - loss: 0.1159\n",
      "Epoch 159/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9887 - loss: 0.0563 \n",
      "Epoch 160/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9484 - loss: 0.1247\n",
      "Epoch 161/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9623 - loss: 0.1173\n",
      "Epoch 162/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9601 - loss: 0.0941\n",
      "Epoch 163/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - binary_accuracy: 0.9754 - loss: 0.0727\n",
      "Epoch 164/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9621 - loss: 0.0942\n",
      "Epoch 165/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9574 - loss: 0.0977    \n",
      "Epoch 166/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9455 - loss: 0.1305\n",
      "Epoch 167/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9762 - loss: 0.0731\n",
      "Epoch 168/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9572 - loss: 0.0923\n",
      "Epoch 169/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9368 - loss: 0.1074\n",
      "Epoch 170/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9666 - loss: 0.0742\n",
      "Epoch 171/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9666 - loss: 0.0678\n",
      "Epoch 172/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9603 - loss: 0.0908\n",
      "Epoch 173/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - binary_accuracy: 0.9590 - loss: 0.1135\n",
      "Epoch 174/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9878 - loss: 0.0620\n",
      "Epoch 175/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9528 - loss: 0.0995\n",
      "Epoch 176/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9324 - loss: 0.1178\n",
      "Epoch 177/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9699 - loss: 0.0762\n",
      "Epoch 178/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9881 - loss: 0.0538\n",
      "Epoch 179/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9426 - loss: 0.1139\n",
      "Epoch 180/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9647 - loss: 0.0764\n",
      "Epoch 181/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9661 - loss: 0.0860\n",
      "Epoch 182/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9621 - loss: 0.0957\n",
      "Epoch 183/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9625 - loss: 0.0817\n",
      "Epoch 184/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9546 - loss: 0.1100\n",
      "Epoch 185/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9442 - loss: 0.1499\n",
      "Epoch 186/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9721 - loss: 0.0764\n",
      "Epoch 187/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9460 - loss: 0.1006\n",
      "Epoch 188/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9863 - loss: 0.0543\n",
      "Epoch 189/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9572 - loss: 0.1202\n",
      "Epoch 190/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9694 - loss: 0.0871\n",
      "Epoch 191/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9441 - loss: 0.1065\n",
      "Epoch 192/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9511 - loss: 0.0997\n",
      "Epoch 193/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9574 - loss: 0.1147\n",
      "Epoch 194/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9770 - loss: 0.0607  \n",
      "Epoch 195/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9475 - loss: 0.0958\n",
      "Epoch 196/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9368 - loss: 0.1792\n",
      "Epoch 197/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9632 - loss: 0.0757\n",
      "Epoch 198/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - binary_accuracy: 0.9812 - loss: 0.0651\n",
      "Epoch 199/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9669 - loss: 0.0626\n",
      "Epoch 200/200\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9494 - loss: 0.1195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x265bbfef6e0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classificador.fit(previsores_treinamento, classe_treinamento, batch_size = 10, epochs = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 1.25575215e-01,  7.70652550e-04,  7.35881999e-02,\n",
      "         4.22095060e-02,  1.98103562e-02, -1.82373039e-02,\n",
      "        -1.56430617e-01,  3.30460556e-02, -3.53248268e-02,\n",
      "        -1.81602478e-01,  1.35559347e-02, -6.44759834e-02,\n",
      "         1.79411411e-01, -6.73603043e-02,  1.50560424e-01,\n",
      "         3.13421376e-02],\n",
      "       [-1.02124907e-01, -1.65894940e-01, -1.98685139e-01,\n",
      "         4.59981529e-04,  1.12626282e-02,  2.14971192e-02,\n",
      "         1.54245928e-01, -1.00997994e-02,  1.93431433e-02,\n",
      "        -3.75629157e-01,  1.53605476e-01, -3.05068307e-02,\n",
      "        -1.33100882e-01,  2.46593580e-01,  2.60386229e-01,\n",
      "        -9.34687778e-02],\n",
      "       [ 3.16634178e-01, -7.50783384e-02,  3.20802480e-01,\n",
      "        -3.50398161e-02, -3.92420329e-02,  2.60410346e-02,\n",
      "        -3.00012022e-01, -4.84254435e-02, -6.47626668e-02,\n",
      "        -1.07513085e-01,  2.05229148e-02, -7.75812641e-02,\n",
      "         3.60383868e-01, -3.14831078e-01,  1.70200899e-01,\n",
      "        -3.90465818e-02],\n",
      "       [ 7.44950175e-02, -1.86656453e-02,  7.84212276e-02,\n",
      "         3.51268016e-02, -9.14389938e-02, -2.68977284e-02,\n",
      "        -5.45446277e-02, -3.56322080e-02, -4.45171893e-02,\n",
      "         7.64553919e-02,  9.14117321e-03, -2.71556303e-02,\n",
      "         4.64122109e-02, -6.29814044e-02,  2.90959980e-02,\n",
      "        -1.17639992e-02],\n",
      "       [-2.97620386e-01, -1.36822551e-01, -1.13386855e-01,\n",
      "        -5.37809134e-02,  2.77438879e-01, -1.06262192e-02,\n",
      "         4.93511558e-02,  3.13964151e-02,  1.07903788e-02,\n",
      "        -1.74310133e-02, -5.21812066e-02, -1.91820599e-02,\n",
      "        -1.60387233e-01,  3.10433097e-02, -9.02054161e-02,\n",
      "        -4.61158827e-02],\n",
      "       [-3.31465811e-01,  1.46164849e-01,  1.89362422e-01,\n",
      "        -4.98505961e-03, -3.53647396e-02, -4.22402620e-02,\n",
      "        -2.55360574e-01, -4.37059589e-02,  1.64934546e-02,\n",
      "         2.08261564e-01, -5.92115596e-02, -1.29849287e-02,\n",
      "        -1.83813572e-01,  1.59093708e-01,  3.00124854e-01,\n",
      "         1.96466759e-01],\n",
      "       [ 5.65074570e-03,  3.74683529e-01,  4.30501774e-02,\n",
      "        -1.66862495e-02,  1.11465499e-01, -1.54692903e-02,\n",
      "        -3.41015048e-02, -3.77205126e-02, -2.05755094e-03,\n",
      "         1.76101536e-01, -2.25210469e-02,  3.13886553e-02,\n",
      "        -1.76461667e-01,  9.74095985e-03, -2.19338804e-01,\n",
      "        -9.66390073e-02],\n",
      "       [-4.37576741e-01,  4.34238985e-02,  8.88591558e-02,\n",
      "         3.15345079e-02,  2.47218400e-01,  1.76650994e-02,\n",
      "        -5.54788560e-02,  1.69243962e-02, -8.87809172e-02,\n",
      "         2.38219947e-01, -3.83661278e-02, -2.77518872e-02,\n",
      "         1.35944963e-01, -2.11264640e-01,  2.06978172e-01,\n",
      "         2.00778451e-02],\n",
      "       [ 6.51083589e-02,  6.28579706e-02, -9.12295729e-02,\n",
      "        -5.18014207e-02,  4.49400783e-01, -4.89044785e-02,\n",
      "         6.04451708e-02, -7.02525442e-03,  1.82883814e-02,\n",
      "        -1.27830118e-01,  3.27051319e-02, -1.64754111e-02,\n",
      "         5.26547842e-02, -3.75822067e-01,  1.92236140e-01,\n",
      "         2.43801162e-01],\n",
      "       [ 6.78234339e-01, -2.21643504e-02, -1.56361237e-01,\n",
      "        -4.74944189e-02, -1.55512959e-01, -2.46930122e-02,\n",
      "         9.55113545e-02, -1.32532511e-02, -1.97432861e-02,\n",
      "        -2.66976953e-01,  1.60386264e-01, -8.31385609e-03,\n",
      "        -3.93193662e-01, -4.80775505e-01, -1.58649430e-01,\n",
      "         4.10985500e-01],\n",
      "       [ 8.67412542e-04,  2.27329895e-01,  2.48560868e-03,\n",
      "         4.08750437e-02, -1.29226208e-01,  2.84608491e-02,\n",
      "        -3.85276116e-02,  4.73946892e-02, -9.23454389e-02,\n",
      "         8.85356218e-03,  1.09734396e-02, -7.69188255e-02,\n",
      "        -8.22060108e-02,  5.23219481e-02,  1.88646212e-01,\n",
      "         5.48071153e-02],\n",
      "       [-7.41653964e-02, -2.56929547e-02,  1.45641034e-02,\n",
      "        -2.14078575e-02,  6.18236698e-02, -4.00131941e-03,\n",
      "         2.92199547e-04, -1.94375589e-02, -1.25508709e-02,\n",
      "        -3.12828660e-01, -6.83804303e-02, -1.11001879e-02,\n",
      "         3.00814342e-02, -2.16198876e-03,  3.23446095e-02,\n",
      "         7.15206638e-02],\n",
      "       [-1.43338339e-02, -4.37235795e-02,  1.36718005e-02,\n",
      "        -2.89931167e-02, -1.56789105e-02, -2.35767718e-02,\n",
      "        -8.90235044e-03, -1.16308848e-03, -1.37510411e-02,\n",
      "        -8.56640935e-02, -1.02888048e-01, -1.76237859e-02,\n",
      "         7.29024922e-03,  1.82665456e-02, -7.91077092e-02,\n",
      "        -4.32225987e-02],\n",
      "       [ 2.97956485e-02, -6.90128480e-04, -3.25330615e-01,\n",
      "        -2.15031076e-02, -1.48881257e-01, -3.35782394e-02,\n",
      "         2.30526626e-02, -1.16647920e-02,  9.91474465e-02,\n",
      "        -1.22224279e-02, -4.36322019e-02,  9.42654833e-02,\n",
      "        -3.20385814e-01,  4.75994591e-03,  4.25363295e-02,\n",
      "         2.19693035e-02],\n",
      "       [-2.05136061e-01, -2.36320235e-02, -5.99862218e-01,\n",
      "         3.03536635e-02,  5.79210483e-02,  3.78461145e-02,\n",
      "         5.19801319e-01, -1.51465619e-02, -1.77233554e-02,\n",
      "         9.58158523e-02,  5.77032343e-02, -2.86993738e-02,\n",
      "        -5.30224383e-01,  6.16489768e-01, -3.73759083e-02,\n",
      "        -1.17600806e-01],\n",
      "       [-7.21610665e-01,  1.53505281e-01, -2.19130665e-01,\n",
      "        -8.32994469e-03,  8.80653709e-02,  1.05918534e-02,\n",
      "         5.04305840e-01, -2.06341371e-02, -4.74625081e-02,\n",
      "         4.95358497e-01, -1.22773916e-01,  2.92506348e-03,\n",
      "        -2.20198750e-01,  3.54597747e-01,  9.07985419e-02,\n",
      "         2.08638668e-01],\n",
      "       [-4.04972404e-01,  1.26564026e-01,  4.21585292e-01,\n",
      "         2.85702180e-02, -4.77678999e-02, -1.58461705e-02,\n",
      "        -3.78852904e-01, -1.76439770e-02, -8.72066617e-02,\n",
      "         7.57940352e-01,  4.12787087e-02, -6.21645823e-02,\n",
      "         4.07242179e-01, -4.43753690e-01,  5.52869797e-01,\n",
      "         7.88614377e-02],\n",
      "       [ 2.56390899e-01, -2.00091913e-01,  5.10484278e-01,\n",
      "         1.33645302e-03, -1.04707681e-01, -3.25174257e-03,\n",
      "        -5.54472864e-01, -5.54548614e-02, -2.00702865e-02,\n",
      "        -4.46185857e-01,  2.33073533e-01,  4.79749264e-03,\n",
      "         5.64540088e-01,  4.43886310e-01,  1.97580501e-01,\n",
      "         1.22367114e-01],\n",
      "       [ 4.84012604e-01,  2.63303500e-02,  4.69462097e-01,\n",
      "        -4.88436744e-02, -3.28422710e-03, -4.68269587e-02,\n",
      "        -4.58946228e-01,  4.39361259e-02, -5.47270384e-03,\n",
      "         2.76367873e-01,  2.61466950e-02, -3.29249240e-02,\n",
      "         5.89449823e-01, -2.13285938e-01,  3.18404168e-01,\n",
      "         5.41823357e-02],\n",
      "       [-4.28298563e-01,  1.03288770e-01, -6.32998109e-01,\n",
      "        -2.52507720e-02, -1.92149784e-02,  4.42784764e-02,\n",
      "         4.44466144e-01,  1.89865772e-02, -6.38530180e-02,\n",
      "         4.48793590e-01,  1.03145793e-01, -1.89670585e-02,\n",
      "        -9.15650189e-01,  8.12406242e-01, -2.95611382e-01,\n",
      "        -5.15937582e-02],\n",
      "       [ 2.59230793e-01, -9.46115032e-02,  2.46048242e-01,\n",
      "        -2.51011886e-02, -4.42765951e-02, -2.86358595e-03,\n",
      "        -2.34948188e-01, -9.45049673e-02, -4.14757244e-02,\n",
      "        -1.49070770e-01,  5.59614375e-02, -3.55256088e-02,\n",
      "         3.17628890e-01, -2.02562779e-01,  1.55342817e-01,\n",
      "        -1.05866883e-02],\n",
      "       [-2.19987303e-01, -1.66003302e-01, -4.76020485e-01,\n",
      "        -1.35243102e-03, -8.29647295e-03, -4.42958735e-02,\n",
      "         2.83933878e-01, -2.14198437e-02, -6.27265573e-02,\n",
      "        -4.34168637e-01,  1.85522959e-01, -3.20548937e-02,\n",
      "        -3.10603082e-01,  4.72661078e-01,  1.40404761e-01,\n",
      "        -1.45882726e-01],\n",
      "       [ 1.75139323e-01, -2.45993845e-02,  1.70879066e-01,\n",
      "         2.93815788e-02, -5.55512086e-02,  3.17359082e-02,\n",
      "        -1.83243930e-01, -8.09841976e-02, -1.22934263e-02,\n",
      "        -1.46951213e-01,  3.40623520e-02, -3.13118249e-02,\n",
      "         1.88351840e-01, -1.67143196e-01,  1.52619809e-01,\n",
      "         4.54911180e-02],\n",
      "       [-6.51268754e-04, -2.46437895e-03, -1.13033973e-01,\n",
      "        -4.33169082e-02, -1.54225707e-01, -2.71348711e-02,\n",
      "         1.51307732e-01, -7.41573563e-03, -4.05483916e-02,\n",
      "         5.98496906e-02, -4.39412780e-02, -3.70754451e-02,\n",
      "        -8.78345892e-02,  8.78259018e-02,  1.42274825e-02,\n",
      "        -1.93115696e-02],\n",
      "       [ 3.44937801e-01, -7.52911493e-02,  2.83740759e-02,\n",
      "        -2.06504501e-02,  1.74698476e-02, -5.07948548e-03,\n",
      "         1.17278270e-01,  8.89694376e-04,  3.63695696e-02,\n",
      "        -1.53335005e-01,  4.11122069e-02,  1.71506517e-02,\n",
      "         3.82190645e-02, -7.26683214e-02, -2.74769872e-01,\n",
      "        -1.78855821e-01],\n",
      "       [-1.36527508e-01, -6.60331473e-02,  1.79876313e-02,\n",
      "         2.90220673e-03, -8.34522769e-03,  4.83332165e-02,\n",
      "        -3.12890038e-02,  8.67530529e-04, -7.22531304e-02,\n",
      "        -1.48701459e-01,  4.97699454e-02, -1.65613496e-03,\n",
      "        -4.04554829e-02, -1.19147189e-02, -8.84734318e-02,\n",
      "        -3.20281237e-02],\n",
      "       [-1.74888253e-01, -8.79687667e-02,  4.86839935e-03,\n",
      "         2.18335353e-02, -4.79068011e-02, -3.11954860e-02,\n",
      "         4.58900910e-03, -7.44476840e-02,  5.32976575e-02,\n",
      "        -2.18285471e-01,  2.35996604e-01, -2.69828248e-03,\n",
      "        -3.05539798e-02, -2.38683492e-01, -2.55047325e-02,\n",
      "        -2.89670266e-02],\n",
      "       [-1.70869678e-01,  3.82242978e-01,  4.25935462e-02,\n",
      "        -2.05721911e-02, -2.54951660e-02,  5.23346663e-03,\n",
      "         1.29923150e-01, -2.75803227e-02, -3.11323125e-02,\n",
      "         1.33298129e-01, -6.59950525e-02, -4.47250381e-02,\n",
      "         2.65035462e-02, -4.53590155e-01,  9.07613412e-02,\n",
      "        -8.82463250e-03],\n",
      "       [-6.52073603e-03, -8.41251668e-03,  2.48841085e-02,\n",
      "        -1.29160509e-02,  9.73786637e-02, -4.14760225e-02,\n",
      "         2.09409837e-02, -2.17813961e-02,  5.10268286e-02,\n",
      "        -1.22299984e-01,  3.09399404e-02,  9.72548723e-02,\n",
      "        -8.10022354e-02,  1.06722759e-02, -7.44650438e-02,\n",
      "         1.23815341e-02],\n",
      "       [-7.40764961e-02, -1.21986484e-02, -3.31491947e-01,\n",
      "        -3.58449779e-02, -6.27296120e-02, -4.20302041e-02,\n",
      "         4.58897799e-01,  6.14119843e-02,  4.62697744e-02,\n",
      "        -6.53538257e-02,  1.73504464e-02, -4.59447429e-02,\n",
      "        -2.26474717e-01,  4.62705672e-01, -1.43309772e-01,\n",
      "        -1.07360236e-01]], dtype=float32), array([ 0.5633032 , -0.15731584,  0.63155293, -0.00527374,  0.10611419,\n",
      "        0.        , -0.7193528 , -0.09111781, -0.01341569, -0.35976595,\n",
      "        0.171202  , -0.02986337,  0.76999104, -0.6553261 ,  0.42864743,\n",
      "       -0.00677967], dtype=float32)]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "pesos0 = classificador.layers[0].get_weights()\n",
    "print(pesos0)\n",
    "print(len(pesos0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000265B94D6C00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 178ms/stepWARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000265B94D6C00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n"
     ]
    }
   ],
   "source": [
    "previsoes = classificador.predict(previsores_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - binary_accuracy: 0.9340 - loss: 0.1326  \n"
     ]
    }
   ],
   "source": [
    "resultado = classificador.evaluate(previsores_teste, classe_teste)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
